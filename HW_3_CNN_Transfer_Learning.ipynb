{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shir2275.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/amitshakarchy/HW_3_CNN/blob/master/HW_3_CNN_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hqBDj7cEz6p0"
      },
      "source": [
        "from keras.layers import BatchNormalization\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from tensorflow.python.keras import Input\n",
        "from tensorflow.python.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow_hub as hub\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.python.keras.callbacks import Callback\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "from tensorflow.python.keras.layers import Dropout, GlobalAveragePooling2D\n",
        "\n",
        "\n",
        "def data(split_another_way=False):\n",
        "    dataset, dataset_info = tfds.load('oxford_flowers102', with_info=True, as_supervised=True,\n",
        "                                      split='train+test+validation')\n",
        "    # base on https://stackoverflow.com/questions/60646972/trouble-with-splitting-data-from-tensorflow-datasets?fbclid=IwAR2_gQUx3Fw07pVoFwRImVsG5skpl1A0B032qMmou1-lXWtkO-JjprLGozY\n",
        "    df_all_length = [i for i, _ in enumerate(dataset)][-1] + 1\n",
        "\n",
        "    train_size = int(0.5 * df_all_length)\n",
        "    val_test_size = int(0.25 * df_all_length)\n",
        "\n",
        "    if split_another_way:\n",
        "        test_val_set = dataset.take(val_test_size * 2)  # [ 1, 2, 3, 4, 5]\n",
        "        training_set = dataset.skip(val_test_size * 2)  # [ 6, 7, 8, 9, 10]\n",
        "        validation_set = test_val_set.skip(val_test_size)\n",
        "        test_set = test_val_set.take(val_test_size)\n",
        "\n",
        "    else:\n",
        "        # split whole dataset\n",
        "        # [ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "        training_set = dataset.take(train_size)  # [ 1, 2, 3, 4, 5]\n",
        "        df_test_val = dataset.skip(train_size)  # [ 6, 7, 8, 9, 10]\n",
        "        validation_set = df_test_val.skip(val_test_size)  # [9, 10]\n",
        "        test_set = df_test_val.take(val_test_size)  # [6, 7, 8]\n",
        "\n",
        "    # Create a training set, a validation set and a test set.\n",
        "    # test_set, training_set, validation_set = dataset['test'], dataset['train'], dataset['validation']\n",
        "    num_training_examples = 0\n",
        "    num_validation_examples = 0\n",
        "    num_test_examples = 0\n",
        "\n",
        "    for example in training_set:\n",
        "        num_training_examples += 1\n",
        "\n",
        "    for example in validation_set:\n",
        "        num_validation_examples += 1\n",
        "\n",
        "    for example in test_set:\n",
        "        num_test_examples += 1\n",
        "\n",
        "    print('Total Number of Training Images: {}'.format(num_training_examples))\n",
        "    print('Total Number of Validation Images: {}'.format(num_validation_examples))\n",
        "    print('Total Number of Test Images: {} \\n'.format(num_test_examples))\n",
        "    # Get the number of classes in the dataset from the dataset info.\n",
        "    num_classes = dataset_info.features['label'].num_classes\n",
        "    print('Total Number of Classes: {}'.format(num_classes))\n",
        "\n",
        "    def format_image(image, label):\n",
        "        if normalization:\n",
        "            image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE)) / 255.0\n",
        "        else:\n",
        "            image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n",
        "        return image, label\n",
        "\n",
        "    train_batches = training_set.cache().shuffle(num_training_examples // 4).map(format_image).batch(\n",
        "        BATCH_SIZE).prefetch(1)\n",
        "\n",
        "    validation_batches = validation_set.cache().map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "    test_batches = test_set.cache().map(format_image).batch(BATCH_SIZE).prefetch(1)\n",
        "\n",
        "    return train_batches, validation_batches, test_batches\n",
        "\n",
        "\n",
        "def get_mobilenet_v2_adapted(hparams):\n",
        "    URL = \"https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4\"\n",
        "    feature_extractor = hub.KerasLayer(URL, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    # Freeze the Pre-Trained Model\n",
        "    feature_extractor.trainable = False\n",
        "\n",
        "    # Attach a classification head\n",
        "    model = tf.keras.Sequential([\n",
        "        feature_extractor,\n",
        "        Dense(hparams[HP_NUM_UNITS], activation='relu'),\n",
        "        Dropout(hparams[HP_DROPOUT]),\n",
        "        layers.Dense(N_CLASSES, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "def get_inceptionv3_adapted(hparams):\n",
        "    input_tensor = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    base_model = InceptionV3(include_top=False,\n",
        "                             weights='imagenet',\n",
        "                             input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "    base_model.trainable = False\n",
        "    bn = BatchNormalization()(input_tensor)\n",
        "    x = base_model(bn)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(hparams[HP_NUM_UNITS], activation='relu')(x)\n",
        "    x = Dropout(hparams[HP_DROPOUT])(x)\n",
        "    output = Dense(N_CLASSES, activation='softmax')(x)\n",
        "    model = Model(input_tensor, output)\n",
        "    return model\n",
        "\n",
        "# Implementation base on https://github.com/keras-team/keras/issues/2548\n",
        "class TestCallback(Callback):\n",
        "    def __init__(self, test_data):\n",
        "        super().__init__()\n",
        "        self.test_data = test_data\n",
        "        self.history_test = {'test_accuracy': [], 'test_loss': []}\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        x = self.test_data.as_numpy_iterator().next()[0]\n",
        "        y = self.test_data.as_numpy_iterator().next()[1]\n",
        "        loss, acc = self.model.evaluate(x, y, verbose=0)\n",
        "        self.history_test['test_accuracy'].append(acc)\n",
        "        self.history_test['test_loss'].append(loss)\n",
        "        print('\\nTesting loss: {}, acc: {}\\n'.format(loss, acc))\n",
        "\n",
        "\n",
        "def plot(history, history_test, session_num):\n",
        "    epochs_range = range(len(history.history['accuracy']))\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.plot(epochs_range, history_test['test_accuracy'], label='Test Accuracy')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.title('Accuracy')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, history.history['loss'], label='Training Loss')\n",
        "    plt.plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
        "    plt.plot(epochs_range, history_test['test_loss'], label='Test Loss')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.title('Loss')\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(str(session_num) + '.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# Press the green button in the gutter to run the script.\n",
        "print(\"Let's go!\")\n",
        "N_CLASSES = 102\n",
        "IMG_SIZE = 224\n",
        "BATCH_SIZE = 32\n",
        "EPOCHS = 10\n",
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([128, 256]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.0, 0.3]))\n",
        "HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "optimizer = 'adam'\n",
        "INPUT_SHAPE = Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "\n",
        "parameters = dict(model='feature_vector', normalization=True, num_units=128, dropout_rate=0.3, split_another_way=False)\n",
        "# model - 'inceptionv3' , 'feature_vector'\n",
        "# normalization - True, False\n",
        "# num_units - 128, 256\n",
        "# dropout - 0.0, 0.3\n",
        "# split_another_way - False, True\n",
        "\n",
        "run_model = parameters['model']\n",
        "normalization = parameters['normalization']\n",
        "num_units = parameters['num_units']\n",
        "dropout_rate = parameters['dropout_rate']\n",
        "train_batches, validation_batches, test_batches = data(parameters['split_another_way'])\n",
        "\n",
        "hparams = {\n",
        "    HP_NUM_UNITS: num_units,\n",
        "    HP_DROPOUT: dropout_rate,\n",
        "    HP_OPTIMIZER: optimizer,\n",
        "}\n",
        "if run_model == 'feature_vector':\n",
        "    model = get_mobilenet_v2_adapted(hparams)\n",
        "else:\n",
        "    model = get_inceptionv3_adapted(hparams)\n",
        "print(f\"-----------------------------------------------------------{run_model} \"\n",
        "      f\"--------------------------------------------------------------\")\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.RMSprop(),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "print({h.name: hparams[h] for h in hparams})\n",
        "\n",
        "# Stop training when there is no improvement in the validation loss for 5 consecutive epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
        "callable_test = TestCallback(test_batches)\n",
        "\n",
        "history = model.fit(train_batches,\n",
        "                    epochs=EPOCHS,\n",
        "                    validation_data=validation_batches,\n",
        "                    callbacks=[early_stopping, callable_test])\n",
        "\n",
        "\n",
        "loss_test, acc_test = model.evaluate(test_batches)\n",
        "str_loss_acc = \"loss_{:.3f}_acc_{:.3f}\".format(loss_test, acc_test)\n",
        "plot(history, callable_test.history_test, str_loss_acc)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}